{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to AI\n",
    "## Text and Speech Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Frequency Analysis\n",
    "\n",
    "#### Load a Text Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Curl to get a document from GitHub and open it\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Moon.txt -o Moon.txt\n",
    "doc1 = open(\"Moon.txt\", \"r\")\n",
    "\n",
    "# Read the document and print its contents\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# remove numeric digits\n",
    "txt = ''.join(c for c in doc1Txt if not c.isdigit())\n",
    "\n",
    "# remove punctuation and make lower case\n",
    "txt = ''.join(c for c in txt if c not in punctuation).lower()\n",
    "\n",
    "# print the normalized text\n",
    "print (txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "# Get the frequency distribution of the words into a data frame\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "print (count_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution as a pareto chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort the data frame by frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "\n",
    "# Display the top 60 words as a bar plot\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get standard stop words from NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Filter out the stop words\n",
    "txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get the frequency distribution of the remaining words\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot the frequency of the top 60 words\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency\n",
    "#### View the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remind ourselves of the first document\n",
    "print(doc1Txt)\n",
    "print(\"------------------------------------------------\")\n",
    "\n",
    "# Get a second document, normalize it, and remove stop words\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Gettysburg.txt -o Gettysburg.txt\n",
    "doc2 = open(\"Gettysburg.txt\", \"r\")\n",
    "doc2Txt = doc2.read()\n",
    "print (doc2Txt)\n",
    "from string import punctuation\n",
    "txt2 = ''.join(c for c in doc2Txt if not c.isdigit())\n",
    "txt2 = ''.join(c for c in txt2 if c not in punctuation).lower()\n",
    "txt2 = ' '.join([word for word in txt2.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "\n",
    "# and a third\n",
    "print(\"------------------------------------------------\")\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Cognitive.txt -o Cognitive.txt\n",
    "doc3 = open(\"Cognitive.txt\", \"r\")\n",
    "doc3Txt = doc3.read()\n",
    "print (doc3Txt)\n",
    "from string import punctuation\n",
    "txt3 = ''.join(c for c in doc3Txt if not c.isdigit())\n",
    "txt3 = ''.join(c for c in txt3 if c not in punctuation).lower()\n",
    "txt3 = ' '.join([word for word in txt3.split() if word not in (stopwords.words('english'))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get TF-IDF Values for the top three words in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install textblob library and define functions for TF-IDF\n",
    "!pip install -U textblob\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.words.count(word) / len(doc.words)\n",
    "\n",
    "def contains(word, docs):\n",
    "    return sum(1 for doc in docs if word in doc.words)\n",
    "\n",
    "def idf(word, docs):\n",
    "    return math.log(len(docs) / (1 + contains(word, docs)))\n",
    "\n",
    "def tfidf(word, doc, docs):\n",
    "    return tf(word,doc) * idf(word, docs)\n",
    "\n",
    "\n",
    "# Create a collection of documents as textblobs\n",
    "doc1 = tb(txt)\n",
    "doc2 = tb(txt2)\n",
    "doc3 = tb(txt3)\n",
    "docs = [doc1, doc2, doc3]\n",
    "\n",
    "# Use TF-IDF to get the three most important words from each document\n",
    "print('-----------------------------------------------------------')\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, doc, docs) for word in doc.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "#### View frequency of unstemmed words from Kennedy's inauguration speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and print text\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/KennedyInaugural.txt -o KennedyInaugural.txt\n",
    "doc4 = open(\"KennedyInaugural.txt\", \"r\")\n",
    "kenTxt = doc4.read()\n",
    "\n",
    "print(kenTxt)\n",
    "\n",
    "# Normalize and remove stop words\n",
    "from string import punctuation\n",
    "kenTxt = ''.join(c for c in kenTxt if not c.isdigit())\n",
    "kenTxt = ''.join(c for c in kenTxt if c not in punctuation).lower()\n",
    "kenTxt = ' '.join([word for word in kenTxt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# Get Frequency distribution\n",
    "words = nltk.tokenize.word_tokenize(kenTxt)\n",
    "fdist = FreqDist(words)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stem the words using the Porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Get the word stems\n",
    "ps = PorterStemmer()\n",
    "stems = [ps.stem(word) for word in words]\n",
    "\n",
    "# Get Frequency distribution\n",
    "fdist = FreqDist(stems)\n",
    "count_frame = pd.DataFrame(fdist, index =[0]).T\n",
    "count_frame.columns = ['Count']\n",
    "\n",
    "# Plot frequency\n",
    "counts = count_frame.sort_values('Count', ascending = False)\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "ax = fig.gca()    \n",
    "counts['Count'][:60].plot(kind = 'bar', ax = ax)\n",
    "ax.set_title('Frequency of the most common words')\n",
    "ax.set_ylabel('Frequency of word')\n",
    "ax.set_xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Analytics\n",
    "\n",
    "https://azure.microsoft.com/en-us/services/cognitive-services/linguistic-analysis-api/\n",
    "\n",
    "#### Use the Linguistics Analytics API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json\n",
    "\n",
    "myText = input('Please enter some text: \\n')\n",
    "\n",
    "headers = {\n",
    "    # Request headers\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': 'YOUR_KEY_HERE',\n",
    "}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "})\n",
    "\n",
    "body = {\n",
    "    \"language\" : \"en\",\n",
    "    \"analyzerIds\" : [\"4fa79af1-f22c-408d-98bb-b7d7aeef7f04\", \"22a6b758-420f-4745-8a3c-46835a67c0d2\"],\n",
    "    \"text\" : myText\n",
    "}\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection('westus.api.cognitive.microsoft.com')\n",
    "    conn.request(\"POST\", \"/linguistics/v1.0/analyze?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    parsed = json.loads(data)\n",
    "    for analyzer in parsed:\n",
    "        print(\"Analyzer: \" + analyzer[\"analyzerId\"])\n",
    "        print(analyzer[\"result\"])\n",
    "        print(\"---------------------------\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analytics\n",
    "#### Create a Text Analytics service in Azure\n",
    "https://portal.azure.com\n",
    "\n",
    "#### Get the region-specific URI and Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textAnalyticsURI = 'REGION.api.cognitive.microsoft.com'\n",
    "textKey = 'YOUR_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Gettysburg and Cognitive documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "\n",
    "# Define the request headers.\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': textKey,\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Define the parameters\n",
    "params = urllib.parse.urlencode({\n",
    "})\n",
    "\n",
    "# Define the request body\n",
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"id\": \"1\",\n",
    "        \"text\": doc2Txt\n",
    "    },\n",
    "    {\n",
    "        \"language\": \"en\",\n",
    "        \"id\": \"2\",\n",
    "        \"text\": doc3Txt\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Execute the REST API call and get the response.\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/keyPhrases?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "\n",
    "    # 'data' contains the JSON response, which includes a collection of documents.\n",
    "    parsed = json.loads(data)\n",
    "    for document in parsed['documents']:\n",
    "        print(\"Document \" + document[\"id\"] + \" key phrases:\")\n",
    "        for phrase in document['keyPhrases']:\n",
    "            print(\"  \" + phrase)\n",
    "        print(\"---------------------------\")\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print('Error:')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body = {\n",
    "  \"documents\": [\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"1\",\n",
    "      \"text\": \"Wow! cognitive services are fantastic.\"\n",
    "    },\n",
    "    {\n",
    "      \"language\": \"en\",\n",
    "      \"id\": \"2\",\n",
    "      \"text\": \"I hate it when computers don't understand me.\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    conn = http.client.HTTPSConnection(textAnalyticsURI)\n",
    "    conn.request(\"POST\", \"/text/analytics/v2.0/sentiment?%s\" % params, str(body), headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode(\"UTF-8\")\n",
    "    parsed = json.loads(data)\n",
    "    \n",
    "    # Get the numeric score for each document\n",
    "    for document in parsed['documents']:\n",
    "        sentiment = \"negative\"\n",
    "        \n",
    "        # if it's more than 0.5, consider the sentiment to be positive.\n",
    "        if document[\"score\"] >= 0.5:\n",
    "            sentiment = \"positive\"\n",
    "        print(\"Document:\" + document[\"id\"] + \" = \" + sentiment)\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech\n",
    "#### Create a Bing Speech API service\n",
    "https://portal.azure.com\n",
    "#### Get the service key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speechKey = 'YOUR_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install SpeechRecognition package\n",
    "https://pypi.python.org/pypi/SpeechRecognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install SpeechRecognition\n",
    "!pip install pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# Read the audio file\n",
    "r = sr.Recognizer()\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    audio = r.listen(source)\n",
    "\n",
    "# transcribe speech using the Bing Speech API\n",
    "try:\n",
    "    transcription = r.recognize_bing(audio, key=speechKey)\n",
    "    print(\"Here's what I heard:\")\n",
    "    print('\"' + transcription + '\"')\n",
    "    \n",
    "except sr.UnknownValueError:\n",
    "    print(\"The audio was unclear\")\n",
    "except sr.RequestError as e:\n",
    "    print (e)\n",
    "    print(\"Something went wrong :-(; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import http.client, urllib.parse, json\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "# Get the input text\n",
    "myText = input('What would you like me to say?: \\n')\n",
    "\n",
    "# The Speech API requires an access token (valid for 10 mins)\n",
    "apiKey = speechKey\n",
    "params = \"\"\n",
    "headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n",
    "AccessTokenHost = \"api.cognitive.microsoft.com\"\n",
    "path = \"/sts/v1.0/issueToken\"\n",
    "\n",
    "# Use the API key to request an access token\n",
    "conn = http.client.HTTPSConnection(AccessTokenHost)\n",
    "conn.request(\"POST\", path, params, headers)\n",
    "response = conn.getresponse()\n",
    "data = response.read()\n",
    "conn.close()\n",
    "accesstoken = data.decode(\"UTF-8\")\n",
    "\n",
    "# Now that we have a token, we can set up the request\n",
    "body = ElementTree.Element('speak', version='1.0')\n",
    "body.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-us')\n",
    "voice = ElementTree.SubElement(body, 'voice')\n",
    "voice.set('{http://www.w3.org/XML/1998/namespace}lang', 'en-US')\n",
    "voice.set('{http://www.w3.org/XML/1998/namespace}gender', 'Male')\n",
    "voice.set('name', 'Microsoft Server Speech Text to Speech Voice (en-US, JessaRUS)')\n",
    "voice.text = myText\n",
    "headers = {\"Content-type\": \"application/ssml+xml\", \n",
    "           \"X-Microsoft-OutputFormat\": \"riff-16khz-16bit-mono-pcm\", \n",
    "           \"Authorization\": \"Bearer \" + accesstoken, \n",
    "           \"X-Search-AppId\": \"07D3234E49CE426DAA29772419F436CA\", \n",
    "           \"X-Search-ClientID\": \"1ECFAE91408841A480F00935DC390960\", \n",
    "           \"User-Agent\": \"TTSForPython\"}\n",
    "\n",
    "#Connect to server to synthesize a wav from the text\n",
    "conn = http.client.HTTPSConnection(\"speech.platform.bing.com\")\n",
    "conn.request(\"POST\", \"/synthesize\", ElementTree.tostring(body), headers)\n",
    "response = conn.getresponse()\n",
    "data = response.read()\n",
    "conn.close()\n",
    "\n",
    "#Play the wav\n",
    "IPython.display.Audio(data, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Translation\n",
    "\n",
    "#### Create a Microsoft Text Translation Service\n",
    "https://portal.azure.com\n",
    "\n",
    "#### Get the service key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transTextKey = \"YOUR_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, http.client, urllib.request, urllib.parse, urllib.error, base64, json, urllib\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "\n",
    "textToTranslate = input('Please enter some text: \\n')\n",
    "fromLangCode = input('What language is this?: \\n') \n",
    "toLangCode = input('To what language would you like it translated?: \\n') \n",
    "\n",
    "try:\n",
    "    # Connect to server to get the Access Token\n",
    "    apiKey = transTextKey\n",
    "    params = \"\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": apiKey}\n",
    "    AccessTokenHost = \"api.cognitive.microsoft.com\"\n",
    "    path = \"/sts/v1.0/issueToken\"\n",
    "\n",
    "    conn = http.client.HTTPSConnection(AccessTokenHost)\n",
    "    conn.request(\"POST\", path, params, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    conn.close()\n",
    "    accesstoken = \"Bearer \" + data.decode(\"UTF-8\")\n",
    "\n",
    "\n",
    "    # Define the request headers.\n",
    "    headers = {\n",
    "        'Authorization': accesstoken\n",
    "    }\n",
    "\n",
    "    # Define the parameters\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"text\": textToTranslate,\n",
    "        \"to\": toLangCode,\n",
    "        \"from\": fromLangCode\n",
    "    })\n",
    "\n",
    "    # Execute the REST API call and get the response.\n",
    "    conn = http.client.HTTPSConnection(\"api.microsofttranslator.com\")\n",
    "    conn.request(\"GET\", \"/V2/Http.svc/Translate?%s\" % params, \"{body}\", headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read()\n",
    "    translation = ElementTree.fromstring(data.decode(\"utf-8\"))\n",
    "    print (translation.text)\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"[Errno {0}] {1}\".format(e.errno, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Understanding Intelligence Service (LUIS)\n",
    "\n",
    "#### Provision LUIS\n",
    "https://portal.azure.com\n",
    "\n",
    "#### Create a LUIS App\n",
    "https://www.luis.ai/\n",
    "\n",
    "Home automation app with a *Light* entity and the following intents:\n",
    "- Light On\n",
    "- Light Off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json \n",
    "\n",
    "# Set up API configuration\n",
    "endpointUrl = \"https://eastus.api.cognitive.microsoft.com/luis/v2.0/apps/7306b6e8-0656-41c1-8dd9-2af977be639d?subscription-key=f181418faa3b4ec6aa753c518d9699a4&verbose=true&timezoneOffset=0&q=\"\n",
    "\n",
    "# prompt for a command\n",
    "command = input('Please enter a command: \\n')\n",
    "\n",
    "# Call the LUIS service and get the JSON response\n",
    "endpoint = endpointUrl + command.replace(\" \",\"+\")\n",
    "response = requests.get(endpoint)\n",
    "data = json.loads(response.content.decode(\"UTF-8\"))\n",
    "\n",
    "# Identify the top scoring intent\n",
    "intent = data[\"topScoringIntent\"][\"intent\"]\n",
    "if (intent == \"Light On\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOn.jpg'\n",
    "elif (intent == \"Light Off\"):\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/LightOff.jpg'\n",
    "else:\n",
    "    img_url = 'https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Dunno.jpg'\n",
    "\n",
    "# Get the appropriate image and show it\n",
    "response = requests.get(img_url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
